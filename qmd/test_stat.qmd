---
title: "Statistical Tests with R - Complete Guide"
execute:
  engine: r 
format: 
  live-revealjs:
    scrollable: true
webr:
  packages: ['ggplot2', 'dplyr', 'effsize']
filters:
  - webr
css: styles.css
---

## Statistical Testing Framework

---

### The Hypothesis Testing Procedure

1. **Formulate hypotheses**:
   - $H_0$: Null hypothesis (no effect)
   - $H_1$: Alternative hypothesis (effect exists)

2. **Choose significance level**: Typically $\alpha = 0.05$

3. **Calculate test statistic**: 
   $$T = \frac{\text{Estimate} - \text{Hypothesized value}}{\text{Standard error}}$$

4. **Determine p-value**: 
   $$p = P(\text{observed data} | H_0 \text{ is true})$$

5. **Make decision**: Reject $H_0$ if $p < \alpha$

---

## One Sample t-Test

### Mathematical Foundation

Tests whether population mean $\mu$ equals specified value $\mu_0$:

**Test statistic**: $t = \dfrac{\bar{x} - \mu_0}{s/\sqrt{n}}$

where:
<table style="font-size: 0.8em; line-height: 1.1;">
<tr><td>$\bar{x}$</td><td>sample mean</td></tr>
<tr><td>$s$</td><td>sample standard deviation</td></tr>
<tr><td>$n$</td><td>sample size</td></tr>
<tr><td>$df = n - 1$</td><td>degrees of freedom</td></tr>
<tr><td></td><td>Data are normally distributed</td></tr>
<tr><td></td><td>Observations are independent</td></tr>
</table>

---

### One Sample t-Test in R

```{webr}
# Example: Test if mean sleep time differs from 8 hours
sleep_data <- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)

# Perform t-test
result <- t.test(sleep_data, mu = 8)

# Detailed output
cat("Sample mean:", round(mean(sleep_data), 2), "\n")
cat("Sample SD:", round(sd(sleep_data), 2), "\n")
cat("t-statistic:", round(result$statistic, 3), "\n")
cat("Degrees of freedom:", result$parameter, "\n")
cat("p-value:", round(result$p.value, 4), "\n")
cat("95% CI: [", round(result$conf.int[1], 2), ",", 
    round(result$conf.int[2], 2), "]\n")
```

---

## Two Sample t-Test

### Independent Samples t-Test

Tests whether two population means are equal:

**Test statistic**:
$$t = \frac{\bar{x}_1 - \bar{x}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

**Pooled standard deviation**:
$$s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$$

**Degrees of freedom**: $df = n_1 + n_2 - 2$

---

### Welch's t-Test (Unequal Variances)

More robust when population variances are unequal:

**Test statistic**:
$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$

**Degrees of freedom** (Satterthwaite approximation):
$$df = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}$$

---

### Two Sample t-Test in R

```{webr}
# Two independent groups
group1 <- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)
group2 <- c(9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4)

# Welch t-test (default)
welch_test <- t.test(group1, group2)
cat("=== WELCH T-TEST ===\n")
print(welch_test)

# Check variance equality
var_test <- var.test(group1, group2)
cat("\n=== F-TEST FOR VARIANCES ===\n")
cat("F-statistic:", round(var_test$statistic, 3), "\n")
cat("p-value:", round(var_test$p.value, 4), "\n")

# Classical t-test (equal variances)
classical_test <- t.test(group1, group2, var.equal = TRUE)
cat("\n=== CLASSICAL T-TEST ===\n")
print(classical_test)
```

---

## Paired t-Test

### Mathematical Formulation

Tests mean difference between paired observations:

**Test statistic**: $t = \dfrac{\bar{d}}{s_d/\sqrt{n}}$

where:
<table style="font-size: 0.70em; line-height: 1.1;">
<tr><td>$d_i = x_{i1} - x_{i2}$</td><td>paired differences</td></tr>
<tr><td>$\bar{d}$</td><td>mean of differences</td></tr>
<tr><td>$s_d$</td><td>standard deviation of differences</td></tr>
<tr><td>$n$</td><td>number of pairs</td></tr>
<tr><td>$df = n - 1$</td><td>degrees of freedom</td></tr>
<tr><td></td><td>Differences are normally distributed</td></tr>
<tr><td></td><td>Pairs are dependent/related</td></tr>
</table>

---

### Paired t-Test in R

```{webr}
# Pre-test and post-test scores
pre_test <- c(69, 77, 35, 34, 87, 45, 95, 83)
post_test <- c(100, 97, 67, 42, 75, 73, 92, 97)

# Calculate differences
differences <- post_test - pre_test

cat("Differences:", differences, "\n")
cat("Mean difference:", round(mean(differences), 2), "\n")
cat("SD of differences:", round(sd(differences), 2), "\n\n")

# Paired t-test
paired_result <- t.test(post_test, pre_test, paired = TRUE)
cat("=== PAIRED T-TEST ===\n")
print(paired_result)

# Compare with unpaired test
unpaired_result <- t.test(post_test, pre_test)
cat("\n=== UNPAIRED T-TEST ===\n")
cat("p-value:", round(unpaired_result$p.value, 4), "\n")
```

---

## Effect Size Measures

### Cohen's d

Standardized mean difference:

**For one sample**: $d = \frac{\bar{x} - \mu_0}{s}$

**For two independent samples**: $d = \frac{\bar{x}_1 - \bar{x}_2}{s_p}$

<table style="font-size: 0.9em; line-height: 1.1;">
<tr><td>$s_p$</td><td>pooled standard deviation</td></tr>
<tr><td>0.2</td><td>Small effect</td></tr>
<tr><td>0.5</td><td>Medium effect</td></tr>
<tr><td>0.8</td><td>Large effect</td></tr>
</table>

---

### Effect Size Calculation in R

```{webr}
# Function to calculate Cohen's d
cohens_d <- function(x, y = NULL, mu = 0) {
  if(is.null(y)) {
    # One sample
    d <- (mean(x) - mu) / sd(x)
  } else {
    # Two samples
    n1 <- length(x)
    n2 <- length(y)
    pooled_sd <- sqrt(((n1-1)*var(x) + (n2-1)*var(y)) / (n1 + n2 - 2))
    d <- (mean(x) - mean(y)) / pooled_sd
  }
  return(d)}
# Examples
group1 <- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)
group2 <- c(9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4)
cat("Cohen's d for two samples:", round(cohens_d(group1, group2), 3), "\n")
# Using effsize package
library(effsize)
d_result <- cohen.d(group1, group2)
cat("Cohen's d (effsize package):", round(d_result$estimate, 3), "\n")
cat("95% CI: [", round(d_result$conf.int[1], 3), ",", 
    round(d_result$conf.int[2], 3), "]\n")
```

---

## Non-Parametric Tests

### Mann-Whitney U Test (Wilcoxon Rank-Sum)

Non-parametric alternative to two-sample t-test:

**Test statistic**:
$$U = \min(U_1, U_2)$$
where:
$$U_1 = n_1n_2 + \frac{n_1(n_1+1)}{2} - R_1$$
$$U_2 = n_1n_2 + \frac{n_2(n_2+1)}{2} - R_2$$

- $R_1, R_2$ = sum of ranks for groups 1 and 2
- Tests whether distributions differ in location

---

### Wilcoxon Signed-Rank Test

Non-parametric alternative to paired t-test:

**Test statistic**:
$$W = \sum_{i=1}^{n} \text{sign}(d_i) \cdot R_i^+$$

where:

| Symbol | Meaning |
|--------|---------|
| $d_i$ | paired differences |
| $R_i^+$ | ranks of absolute differences |
| | Tests whether median difference equals zero |
---

### Non-Parametric Tests in R

```{webr}
# Mann-Whitney U test (unpaired)
group1 <- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)
group2 <- c(9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4)

cat("=== MANN-WHITNEY U TEST ===\n")
mw_test <- wilcox.test(group1, group2)
print(mw_test)

# Wilcoxon signed-rank test (paired)
pre_test <- c(69, 77, 35, 34, 87, 45, 95, 83)
post_test <- c(100, 97, 67, 42, 75, 73, 92, 97)

cat("\n=== WILCOXON SIGNED-RANK TEST ===\n")
wsr_test <- wilcox.test(pre_test, post_test, paired = TRUE)
print(wsr_test)

# Calculate ranks manually for understanding
combined <- c(group1, group2)
ranks <- rank(combined)
cat("\nRanks for combined data:\n")
print(ranks)
```

---

## Chi-Square Tests

### Chi-Square Goodness of Fit Test

Tests whether observed frequencies match expected distribution:

**Test statistic**:
$$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$$

where:
<table style="font-size: 0.99em; line-height: 1;">
<tr><td>$O_i$</td><td>observed frequency in category i</td></tr>
<tr><td>$E_i$</td><td>expected frequency in category i</td></tr>
<tr><td>$df$</td><td>$k - 1$</td></tr>
<tr><td>$k$</td><td># where k = number of categories</td></tr>
</table>
---

### Chi-Square Test of Independence

Tests association between two categorical variables:

**Test statistic**:
$$\chi^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

**Expected frequencies**:
$$E_{ij} = \frac{(row\ total) \times (column\ total)}{grand\ total}$$

**Degrees of freedom**:
$$df = (r-1)(c-1)$$

---

### Chi-Square Tests in R

```{webr}
# Chi-square goodness of fit
observed <- c(25, 30, 45)  # Observed counts
expected_prop <- c(0.25, 0.35, 0.40)  # Expected proportions

cat("=== CHI-SQUARE GOODNESS OF FIT ===\n")
chisq_gof <- chisq.test(observed, p = expected_prop)
print(chisq_gof)

# Chi-square test of independence
contingency_table <- matrix(c(30, 20, 15, 35), nrow = 2,
                           dimnames = list(Gender = c("Male", "Female"),
                                           Preference = c("A", "B")))

cat("\n=== CONTINGENCY TABLE ===\n")
print(contingency_table)

cat("\n=== CHI-SQUARE TEST OF INDEPENDENCE ===\n")
chisq_ind <- chisq.test(contingency_table)
print(chisq_ind)

# Expected frequencies
cat("\nExpected frequencies:\n")
print(round(chisq_ind$expected, 2))
```

---

## Fisher's Exact Test

### Mathematical Basis

Exact test for 2×2 contingency tables:

**Probability of observed configuration**:
$$p = \frac{\binom{a+b}{a} \binom{c+d}{c} \binom{a+c}{a} \binom{b+d}{b}}{\binom{n}{a+b}}$$

where the table is:
```
         | Col1 | Col2 | Total
Row1     |  a   |  b   | a+b
Row2     |  c   |  d   | c+d
Total    | a+c  | b+d  | n
```

**Use when**: Expected frequencies < 5 or small sample sizes

---

### Fisher's Test in R

```{webr}
# Tea tasting experiment
tea_matrix <- matrix(c(3, 1, 1, 3), nrow = 2,
                    dimnames = list(Actual = c("Milk first", "Tea first"),
                                    Guess = c("Milk first", "Tea first")))

cat("=== TEA TASTING EXPERIMENT ===\n")
print(tea_matrix)

cat("\n=== FISHER'S EXACT TEST ===\n")
fisher_two_sided <- fisher.test(tea_matrix)
cat("Two-sided p-value:", round(fisher_two_sided$p.value, 4), "\n")

fisher_greater <- fisher.test(tea_matrix, alternative = "greater")
cat("One-sided (greater) p-value:", round(fisher_greater$p.value, 4), "\n")

fisher_less <- fisher.test(tea_matrix, alternative = "less")
cat("One-sided (less) p-value:", round(fisher_less$p.value, 4), "\n")

# Compare with chi-square
cat("\n=== CHI-SQUARE TEST ===\n")
chisq_tea <- chisq.test(tea_matrix)
print(chisq_tea)
```

---

## ANOVA (Analysis of Variance)

### One-Way ANOVA

Tests equality of means across 3+ groups:

**F-statistic**: $F = \frac{MS_{B}}{MS_{W}}$

where:
<table style="font-size: 0.8em; line-height: 1.1;">
<tr><td><strong>MSB</strong></td><td>$MS_B = \frac{SS_B}{df_B}$</td><td> $df_{B} = k - 1$</td></tr>
<tr><td><strong>MSW</strong></td><td>$MS_W = \frac{SS_W}{df_W}$</td><td>$df_{W} = N - k$</td></tr>
<tr><td><strong>SSB</strong></td><td>$SS_B = \sum n_i(\bar{x}_i - \bar{x})^2$</td><td></td></tr>
<tr><td><strong>SSW</strong></td><td>$SS_W = \sum \sum (x_{ij} - \bar{x}_i)^2$</td><td></td></tr>
</table>

- B = Between

- W = Within

---

### ANOVA in R

```{webr}
# Three groups example
group1 <- c(23, 25, 28, 30, 32)
group2 <- c(19, 22, 24, 27, 29)
group3 <- c(15, 18, 20, 22, 25)

# Combine data
data_anova <- data.frame(
  value = c(group1, group2, group3),
  group = factor(rep(1:3, each = 5))
)

cat("=== ONE-WAY ANOVA ===\n")
anova_result <- aov(value ~ group, data = data_anova)
print(summary(anova_result))

# Post-hoc tests
cat("\n=== TUKEY HSD POST-HOC TEST ===\n")
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)

# Assumption checking
cat("\nGroup means:\n")
print(tapply(data_anova$value, data_anova$group, mean))
```

---

## Assumption Checking

### Normality Tests

```{webr}
# Shapiro-Wilk test for normality
data_normal <- rnorm(50, mean = 100, sd = 15)
data_nonnormal <- rexp(50, rate = 0.1)

cat("=== SHAPIRO-WILK NORMALITY TEST ===\n")
cat("Normal data p-value:", 
    round(shapiro.test(data_normal)$p.value, 4), "\n")
cat("Non-normal data p-value:", 
    round(shapiro.test(data_nonnormal)$p.value, 4), "\n")

# QQ plots for visual inspection
par(mfrow = c(1, 2))
qqnorm(data_normal, main = "Normal Data")
qqline(data_normal)
qqnorm(data_nonnormal, main = "Non-Normal Data")
qqline(data_nonnormal)
```

---

### Homogeneity of Variance

```{webr}
# Levene's test for equal variances
library(car)

group1 <- c(23, 25, 28, 30, 32)
group2 <- c(19, 22, 24, 27, 29)
group3 <- c(15, 18, 20, 22, 25)

data_levene <- data.frame(
  value = c(group1, group2, group3),
  group = factor(rep(1:3, each = 5))
)

cat("=== LEVENE'S TEST FOR HOMOGENEITY ===\n")
levene_test <- leveneTest(value ~ group, data = data_levene)
print(levene_test)

# Bartlett's test (more sensitive to non-normality)
cat("\n=== BARTLETT'S TEST ===\n")
bartlett_test <- bartlett.test(value ~ group, data = data_levene)
print(bartlett_test)
```

---

## Power Analysis

### Sample Size Calculation

**For t-test**:
$$n = \left( \frac{(z_{1-\alpha/2} + z_{1-\beta}) \cdot \sigma}{\delta} \right)^2$$

where:

<table style="font-size: 0.9em; line-height: 1;">
<tr><td>$\alpha$</td><td>sig. level</td></tr>
<tr><td>$\beta$</td><td>Type II error rate</td></tr>
<tr><td>$1-\beta$</td><td>power</td></tr>
<tr><td>$\delta$</td><td>effect size</td></tr>
<tr><td>$\sigma$</td><td>standard deviation</td></tr>
</table>
---

### Power Analysis in R

```{webr}
# Power analysis for t-test
cat("=== POWER ANALYSIS ===\n")

# For two-sample t-test
power_result <- power.t.test(
  n = NULL,           # Solve for sample size
  delta = 5,          # Difference in means
  sd = 10,            # Standard deviation
  sig.level = 0.05,   # Significance level
  power = 0.80,       # Desired power
  type = "two.sample"
)

cat("Required sample size per group:", ceiling(power_result$n), "\n")

# For different effect sizes
effect_sizes <- c(0.2, 0.5, 0.8)
for(d in effect_sizes) {
  power_d <- power.t.test(delta = d * 10, sd = 10, power = 0.80)
  cat("Effect size", d, "-> n =", ceiling(power_d$n), "per group\n")
}
```

---

## Summary Table of Tests

| Test | Purpose | R Function | Key Assumptions |
|------|---------|------------|-----------------|
| One-sample t-test | Compare mean to value | `t.test(x, mu)` | Normality, independence |
| Two-sample t-test | Compare two means | `t.test(x, y)` | Normality, equal variances* |
| Paired t-test | Compare paired means | `t.test(x, y, paired=TRUE)` | Normality of differences |
| Mann-Whitney | Non-parametric two samples | `wilcox.test(x, y)` | Independent samples |
| Wilcoxon signed-rank | Non-parametric paired | `wilcox.test(x, y, paired=TRUE)` | Paired data |
| Chi-square | Categorical association | `chisq.test(table)` | Expected frequencies ≥ 5 |
| Fisher's exact | Small contingency tables | `fisher.test(table)` | Fixed margins |
| ANOVA | Compare 3+ means | `aov(y ~ group)` | Normality, equal variances |

*Welch's t-test doesn't assume equal variances

---

## Best Practices

1. **Always check assumptions** before interpreting results
2. **Use non-parametric tests** when assumptions are violated
3. **Report effect sizes** along with p-values
4. **Consider multiple testing corrections** when appropriate
5. **Use confidence intervals** to understand precision
6. **Visualize your data** before running tests

```{webr}
# Example of comprehensive analysis
cat("=== COMPREHENSIVE ANALYSIS EXAMPLE ===\n")
data <- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)

cat("Descriptive statistics:\n")
cat("Mean:", round(mean(data), 2), "\n")
cat("SD:", round(sd(data), 2), "\n")
cat("95% CI: [", round(mean(data) - 1.96*sd(data)/sqrt(length(data)), 2), 
    ",", round(mean(data) + 1.96*sd(data)/sqrt(length(data)), 2), "]\n")

# Statistical test
test_result <- t.test(data, mu = 8)
cat("\nStatistical test:\n")
cat("t(", test_result$parameter, ") =", round(test_result$statistic, 2), 
    ", p =", round(test_result$p.value, 4), "\n")

# Effect size
cohens_d <- (mean(data) - 8) / sd(data)
cat("Effect size (Cohen's d):", round(cohens_d, 3), "\n")
```
